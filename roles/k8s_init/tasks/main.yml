---
#########################################################################
# tested on centos 7
# 2017-06-30
# created by : lvzhijun
# 1.init k8s cluster
########################################################################
#you can make repo by yourself , pls follow http://www.jianshu.com/p/8ce11f947410 , step 1
- name: config repo
  template: src=k8s.repo dest=/etc/yum.repos.d/k8s.repo owner=root group=root mode=0640
# install kubeadm in all hosts
- name: install kubeadm
  yum: name=kubectl-{{ k8s_version }},kubelet-{{ k8s_version }},kubeadm-{{ k8s_version }} state=present

- name: install python-pip
  yum: name=python-pip state=present

- name: install docker-py
  pip: name=docker-py

- name: docker pull quay.io/coreos/hyperkube
  docker_image:
    name: "{{ KUBE_HYPERKUBE_IMAGE }}"
  when: inventory_hostname in groups["master"]

- name: change kubelet.service add pod-infra-container-image
  template: src=kubelet.service dest=/etc/systemd/system/kubelet.service mode=0755

- name: change kubeadm.conf cgroup-driver
  template: src=10-kubeadm.conf dest=/etc/systemd/system/kubelet.service.d/10-kubeadm.conf mode=0755

- name: systemd reload
  systemd:
     daemon_reload: yes
     name: kubelet
      
- name: sysctl 
  sysctl: name="net.bridge.bridge-nf-call-iptables" value=1 sysctl_set=yes state=present reload=yes

- name: enable services
  service: name={{ item }} state=started enabled=yes
  with_items:
        - kubelet
        - docker

# run kubeadm in commander
- name: run kubeadm
  shell: KUBE_ETCD_IMAGE={{ KUBE_ETCD_IMAGE }} KUBE_REPO_PREFIX={{ KUBE_REPO_PREFIX }} KUBE_HYPERKUBE_IMAGE={{ KUBE_HYPERKUBE_IMAGE }} kubeadm init  --token {{ kubeadm_token }}
  when: inventory_hostname in groups["commander"]  and  network_provider  != 'flannel'

- name: run kubeadm with flannel
  shell: KUBE_ETCD_IMAGE={{ KUBE_ETCD_IMAGE }} KUBE_REPO_PREFIX={{ KUBE_REPO_PREFIX }} KUBE_HYPERKUBE_IMAGE={{ KUBE_HYPERKUBE_IMAGE }} kubeadm init  --token {{ kubeadm_token }} --pod-network-cidr={{ flannel_network_cidr }}
  when: inventory_hostname in groups["commander"]  and  network_provider  == 'flannel'

- name: add ip addr for commander
  shell: ip addr add {{ k8s_api_vip }} dev {{ ka_interface }}
  when: inventory_hostname in groups["commander"]  and k8s_cluster == true

- name: run kubeadm when k8s_cluster
  shell: KUBE_ETCD_IMAGE={{ KUBE_ETCD_IMAGE }} KUBE_REPO_PREFIX={{ KUBE_REPO_PREFIX }} KUBE_HYPERKUBE_IMAGE={{ KUBE_HYPERKUBE_IMAGE }} kubeadm init  --token {{ kubeadm_token }} --apiserver-advertise-address {{ k8s_api_vip }}
  when: inventory_hostname in groups["commander"]  and  network_provider  != 'flannel' and k8s_cluster == true

- name: run kubeadm with flannel when k8s_cluster
  shell: KUBE_ETCD_IMAGE={{ KUBE_ETCD_IMAGE }} KUBE_REPO_PREFIX={{ KUBE_REPO_PREFIX }} KUBE_HYPERKUBE_IMAGE={{ KUBE_HYPERKUBE_IMAGE }} kubeadm init  --token {{ kubeadm_token }} --pod-network-cidr={{ flannel_network_cidr }} --apiserver-advertise-address {{ k8s_api_vip }}
  when: inventory_hostname in groups["commander"]  and  network_provider  == 'flannel' and k8s_cluster == true


- name: kubeadm copy admin.conf
  shell: cp /etc/kubernetes/admin.conf $HOME/ && chown $(id -u):$(id -g) $HOME/admin.conf && export KUBECONFIG=$HOME/admin.conf && echo 'export KUBECONFIG=$HOME/admin.conf' >> $HOME/.bash_profile
  when: inventory_hostname in groups["commander"]

 #todo : wait for ource $HOME/.bash_profile && kubectl get nodes, if not then exit
- name: test kubectl get nodes
  shell: source $HOME/.bash_profile && kubectl get nodes
  register: result_get_nodes
  #until: result.stdout.find("NotReady") != -1
  #retries: 2
  #delay: 10
  when: inventory_hostname in groups["commander"]

- name: test kubectl get nodes result
  shell: test {{ result_get_nodes.rc}} -eq 0
  when: inventory_hostname in groups["commander"]
  
- name: k8s_init mkdir
  file: path={{ k8s_temp_dir }} state=directory mode=755
  when: inventory_hostname in groups["commander"]

- name: copy flannel config file
  template: src={{ item }} dest={{ k8s_temp_dir }} owner=root group=root mode=0640
  with_items:
       - kube-flannel-rbac.yml
       - kube-flannel.yml
  when: inventory_hostname in groups["commander"]  and  network_provider  == 'flannel'

- name: copy calico config file
  template: src=calico.yaml dest={{ k8s_temp_dir }} owner=root group=root mode=0640
  when: inventory_hostname in groups["commander"]  and  network_provider  == 'calico'

- name: kubectl create flannel
  shell: source $HOME/.bash_profile && kubectl create -f {{ k8s_temp_dir }}/kube-flannel-rbac.yml && sleep 3 && kubectl create -f {{ k8s_temp_dir }}/kube-flannel.yml
  when: inventory_hostname in groups["commander"]  and  network_provider  == 'flannel'
       
- name: kubectl create calico
  shell: source $HOME/.bash_profile && kubectl create -f {{ k8s_temp_dir }}/calico.yaml
  when: inventory_hostname in groups["commander"]  and  network_provider  == 'calico'

- name: kubectl create weave
  shell: source $HOME/.bash_profile && kubectl apply -n kube-system -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
  when: inventory_hostname in groups["commander"]  and  network_provider  == 'weave'

- name: kubeadm join
  shell: kubeadm join --token 1732e8.da2263dfad7b89b2 {{ groups['commander'][0] }}:6443
  when:  inventory_hostname not in groups["commander"]

- name: copy heapster yaml
  template: src={{ item }} dest={{ k8s_temp_dir }} owner=root group=root mode=0640
  with_items:
       - heapster.yaml
       - heapster-rbac.yaml
       - influxdb.yaml
       - grafana.yaml
  when:  inventory_hostname in groups["commander"]
  tags:
       - heapster

- name: kubectl create heapster yaml
  shell: source $HOME/.bash_profile && kubectl create -f {{ k8s_temp_dir }}/{{ item }}
  with_items:
       - heapster.yaml
       - heapster-rbac.yaml
       - influxdb.yaml
       - grafana.yaml
  when:  inventory_hostname in groups["commander"]
  tags:
       - heapster

- name: copy dashboard yaml
  template: src=kubernetes-dashboard.yaml dest={{ k8s_temp_dir }} owner=root group=root mode=0640
  when:  inventory_hostname in groups["commander"]
  tags:
       - dashboard

- name: kubectl create dashboard yaml
  shell: source $HOME/.bash_profile && kubectl create -f {{ k8s_temp_dir }}/kubernetes-dashboard.yaml
  when:  inventory_hostname in groups["commander"]
  tags:
       - dashboard

- name: kubectl proxy
  shell: source $HOME/.bash_profile && nohup kubectl proxy --address={{ ansible_nodename }} --accept-hosts='.*' &
  when:  inventory_hostname in groups["commander"] and kubectl_proxy == 'enable'

- name: copy efk yaml
  template: src={{ item }} dest={{ k8s_temp_dir }} owner=root group=root mode=0640
  with_items:
       - es-clusterrole.yaml
       - es-clusterrolebinding.yaml
       - es-controller.yaml
       - es-service.yaml
       - es-serviceaccount.yaml
       - fluentd-es-clusterrole.yaml
       - fluentd-es-clusterrolebinding.yaml
       - fluentd-es-ds.yaml
       - fluentd-es-serviceaccount.yaml
       - kibana-controller.yaml
       - kibana-service.yaml
  when:  inventory_hostname in groups["commander"]
  tags:
       - efk

- name: kubectl create ekf yaml
  shell: source $HOME/.bash_profile && kubectl create -f {{ k8s_temp_dir }}/{{ item }}
  with_items:
       - es-clusterrole.yaml
       - es-clusterrolebinding.yaml
       - es-controller.yaml
       - es-service.yaml
       - es-serviceaccount.yaml
       - fluentd-es-clusterrole.yaml
       - fluentd-es-clusterrolebinding.yaml
       - fluentd-es-ds.yaml
       - fluentd-es-serviceaccount.yaml
       - kibana-controller.yaml
       - kibana-service.yaml
  when:  inventory_hostname in groups["commander"]
  tags:
       - efk

- name: generate kube-label
  local_action: template src=kube-label.j2 dest=/tmp  mode=0644
  when: inventory_hostname in groups["master"]
  tags:
       - efk_label

- name: generate kube-label
  copy: src=/tmp/kube-label.j2 dest=/tmp  mode=0644
  when: inventory_hostname in groups["commander"]
  tags:
       - efk_label

- name: kubectl label efk
  shell: source $HOME/.bash_profile && bash /tmp/kube-label.j2
  when:  inventory_hostname in groups["commander"]
  tags:
       - efk_label
#todo
- include: k8s_ha.yml
  when: k8s_cluster == true


# changed single etcd to cluster etcd
# apiserver
# controller scheduler
# ===
# ingress

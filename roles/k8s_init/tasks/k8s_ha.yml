---
- name: install etcd expect screen
  yum: name={{ item }} state=present
  with_items:
      - etcd
      - expect
      - screen
  when:  inventory_hostname in groups["commander"]

- name: install nginx keepalive
  yum: name={{ item }} state=present
  with_items:
      - keepalived
  when:  inventory_hostname in groups["master"]

- name: copy etc_make_mirror scripts
  copy: src=do_make_mirror.exp dest={{ k8s_temp_dir }} owner=root mode=754
  when:  inventory_hostname in groups["commander"]
  tags: test

- name: copy etc_make_mirror scripts
  template: src=etcd.sh dest={{ k8s_temp_dir }} owner=root mode=754
  when:  inventory_hostname in groups["commander"]
  tags: test

- name: check all pods are running __WARNING__ if too many pods in ContainerCreating, you can download docker images first, and then rerun it
  shell: source $HOME/.bash_profile && test `kubectl get pods -n kube-system |grep -v Running |wc -l` -eq 1
  register: result
  until: result.rc == 0
  retries: 60
  delay: 20
  when:  inventory_hostname in groups["commander"]
  tags: test

- name: stop apiserver and do etcd_make_mirror scripts
  shell: mv /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/ && sleep 10 && cd {{ k8s_temp_dir }} && ./do_make_mirror.exp
  when:  inventory_hostname in groups["commander"]
  tags: test

- name: change port in kube-apiserver.yaml
  shell: sed -i 's/127.0.0.1:2379/127\.0\.0\.1:{{ etcd_port }}/' /etc/kubernetes/kube-apiserver.yaml && mv /etc/kubernetes/kube-apiserver.yaml /etc/kubernetes/manifests/
  when:  inventory_hostname in groups["commander"]
  tags: test

- name: wait for apiserver
  wait_for: host={{ inventory_hostname }} port=6443 delay=10
  when:  inventory_hostname in groups["commander"]
  tags: test

- name: stop singel etcd
  shell: mv /etc/kubernetes/manifests/etcd.yaml /etc/kubernetes/ 
  when:  inventory_hostname in groups["commander"]
  tags: test

- name:  make python simplehttpserver for get pki files
  shell: cd /etc/kubernetes/ && screen -d -m python -m SimpleHTTPServer 18888 
  when:  inventory_hostname in groups["commander"]
  tags: testapi

- name: wait for simplehttpserver
  wait_for: host={{ inventory_hostname }} port=18888 delay=10
  when:  inventory_hostname in groups["commander"]
  tags: testapi

- name: download pki files
  get_url:
      url: http://{{ groups["commander"][0] }}:18888/{{ item }}
      dest: /etc/kubernetes/pki/
      mode: 0600
  with_items:
        - pki/apiserver.crt
        - pki/apiserver.key
        - pki/apiserver-kubelet-client.crt
        - pki/apiserver-kubelet-client.key
        - pki/ca.key
        - pki/front-proxy-ca.crt
        - pki/front-proxy-ca.key
        - pki/front-proxy-client.crt
        - pki/front-proxy-client.key
        - pki/sa.key
        - pki/sa.pub
  when:  inventory_hostname in groups["slave"]
  tags: testapi

- name: download conf files
  get_url: 
      url: http://{{ groups["commander"][0] }}:18888/{{ item }}
      dest: /etc/kubernetes/
  with_items:
        - admin.conf
        - controller-manager.conf
        - scheduler.conf
        - manifests/kube-apiserver.yaml
        - manifests/kube-controller-manager.yaml
        - manifests/kube-scheduler.yaml
  when:  inventory_hostname in groups["slave"]
  tags: testapi

- name: stop python simpleserver
  shell: kill `ps -ef |grep SimpleHTTPServer |awk '{print $2}'`
  ignore_errors: True
  when:  inventory_hostname in groups["commander"]
  tags: testapi

- name: create pki files on slave nodes
  shell: cd /etc/kubernetes/pki/ && openssl genrsa -out apiserver-{{ inventory_hostname }}.key 2048 && openssl req -new -key apiserver-{{ inventory_hostname }}.key -subj "/CN=kube-apiserver," -out apiserver-{{ inventory_hostname }}.csr
  when:  inventory_hostname in groups["slave"]
  tags: createpki

- name: create extfile
  template: src=apiserver-extfile.ext dest=/etc/kubernetes/pki/ owner=root mode=600
  when:  inventory_hostname in groups["slave"]
  tags: createpki

- name: create crt file
  shell: cd /etc/kubernetes/pki/ && openssl x509 -req -in apiserver-{{ inventory_hostname }}.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out apiserver-{{ inventory_hostname }}.crt -days 365 -extfile apiserver-extfile.ext
  when:  inventory_hostname in groups["slave"]
  tags: createpki

- name: change key files in apiserver.yaml
  shell: sed -i 's/apiserver.key/apiserver-{{ inventory_hostname }}\.key/' /etc/kubernetes/kube-apiserver.yaml 
  when:  inventory_hostname in groups["slave"]
  tags: createpki

- name: change key files in apiserver.yaml and start apiserver on localhost
  shell: sed -i 's/apiserver.crt/apiserver-{{ inventory_hostname  }}\.crt/' /etc/kubernetes/kube-apiserver.yaml && mv /etc/kubernetes/kube-apiserver.yaml /etc/kubernetes/manifests/
  when:  inventory_hostname in groups["slave"]
  tags: createpki

- name: check all pods are running
  shell: source $HOME/.bash_profile && test `kubectl get pods -n kube-system |grep -v Running |wc -l` -eq 1
  register: result
  until: result.rc == 0
  retries: 50
  delay: 10
  when:  inventory_hostname in groups["commander"]
  tags: createpki
# =======
- name: change apiserver address in kubelet.yaml
  shell: sed -i 's/server.*6443/server\:\ https\:\/\/{{ inventory_hostname }}:6443/' /etc/kubernetes/kubelet.conf 
  when:  inventory_hostname in groups["slave"]
  tags: kubelet

- name: restart kubelet
  service: name=kubelet state=restarted
  when:  inventory_hostname in groups["slave"]
  tags: kubelet

- name: check all nodes are running
  shell: source $HOME/.bash_profile && test `kubectl get nodes |grep NotReady | wc -l` -eq 0
  register: result
  until: result.rc == 0
  retries: 50
  delay: 10
  when:  inventory_hostname in groups["commander"]
  tags: kubelet
# ======
- name: change apiserver address in scheduler.conf
  shell: sed -i 's/server.*6443/server\:\ https\:\/\/{{ inventory_hostname }}:6443/' /etc/kubernetes/scheduler.conf && mv /etc/kubernetes/kube-scheduler.yaml /etc/kubernetes/manifests
  when:  inventory_hostname in groups["slave"]
  tags: scheduler

- name: check all scheduler are running
  shell: source $HOME/.bash_profile && test `kubectl get pods -n kube-system |grep scheduler |grep -v Running | wc -l` -eq 0
  register: result
  until: result.rc == 0
  retries: 50
  delay: 10
  when:  inventory_hostname in groups["commander"]
  tags: scheduler

# ======
- name: change apiserver address in controller-manager.conf.conf
  shell: sed -i 's/server.*6443/server\:\ https\:\/\/{{ inventory_hostname }}:6443/' /etc/kubernetes/controller-manager.conf && mv /etc/kubernetes/kube-controller-manager.yaml /etc/kubernetes/manifests
  when:  inventory_hostname in groups["slave"]
  tags: controller-manager

- name: check all controller-manager are running
  shell: source $HOME/.bash_profile && test `kubectl get pods -n kube-system |grep controller-manager |grep -v Running | wc -l` -eq 0
  register: result
  until: result.rc == 0
  retries: 50
  delay: 10
  when:  inventory_hostname in groups["commander"]
  tags: controller-manager
# =======
- name: copy admin.conf
  shell: sed -i 's/server.*6443/server\:\ https\:\/\/{{ inventory_hostname }}:6443/' /etc/kubernetes/admin.conf && cp /etc/kubernetes/admin.conf $HOME && chmod 600 $HOME/admin.conf
  when:  inventory_hostname in groups["slave"]
  tags: label-master

- name: set bash_profile
  shell: echo 'export KUBECONFIG=$HOME/admin.conf' >> $HOME/.bash_profile 
  when:  inventory_hostname in groups["slave"]
  tags: label-master

#k8s_master_scheduled: true
- name: label node noschedule
  shell: source $HOME/.bash_profile && kubectl label node {{ ansible_nodename }} node-role.kubernetes.io/master=
  when:  inventory_hostname in groups["slave"] and k8s_master_scheduled != true
  tags: label-master

- name: taint node
  shell: source $HOME/.bash_profile && kubectl taint node {{ ansible_nodename }} node-role.kubernetes.io/master=:NoSchedule
  when:  inventory_hostname in groups["slave"] and k8s_master_scheduled != true
  tags: label-master

- name: install nginx with-stream
  shell: echo 'todo'

#nginx lb and keepalive
#kubectl get configmap/kube-proxy -n kube-system -o yam   and edit
#then restart it 
#=====================
# restart kubectl proxy : kubectl proxy ok?
